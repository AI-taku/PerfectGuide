{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK 패키지를 이용해서 수행 해볼 것임.\n",
    "\n",
    "굉장히 유명한 패키지. NLP 패키지의 시조새 같은, 굉장히 많은 내용을 담고 있고 아카데믹하게 학자들이 고민한 내용들이 많이 들어가 있음.\n",
    "\n",
    "그 안에 들어가있는 굉장히 유용한 말뭉치 사전들, 그런것들이 굉장히 많다. 기반으로 다양한 NLP 테스트를 해볼 수 있는 것을 제공해줌.\n",
    "\n",
    "지금은 NLTK가 딥러닝이나 이런 모델들과는 시대적으로 뒤쳐져서 많이 사용하진 않지만 딥러닝 기반이 나오기전까지만 하더라도\n",
    "\n",
    "NLTK는 기본적으로 생각하고 가는 패키지였었다. \n",
    "\n",
    "이걸 직접 쓰는 경우는 거의 없을 것임. 굳이 이걸 소개하는 이유는 개념상으로 조금 쉽게 이해하기 위해서 이런 패키지로 이러이러한 것들을\n",
    "\n",
    "수행한다라는걸 보기 위해서임. 직접 이 코드를 쓸 일은 없을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문장 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.  \\\n",
    "              You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면, 마침표 기준으로 3개가 나뉘었다. \\ 없이도 3개로 나뉜다. \\를 한 이유는 인터프리터 기준으로 파이썬이 인지하게 하려고 했을 뿐 의미없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단어 토큰화**\n",
    "\n",
    "문장이 하나 주어졌으면 공백으로 보통 토크나이즈를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**여러 문장들에 대한 단어 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 2)\n",
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords 제거\n",
    "\n",
    "Stopwords는 분석적으로 큰 의미가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK 는 패키지가 워낙 커서 필요한걸 받아가면서 해야한다.\n",
    "\n",
    "corpus(말뭉치)라는 패키지는 굉장히 많은 연구용 말뭉치를 가지고 있다. \n",
    "\n",
    "그 중에 필요한 stopwords의 words 중 neglish의 스탑워즈 갯수를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 코드를 직접 쓸 일은 없을 것임. NLP 패키지에 파라미터만 넣어두면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\esaw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>텍스트의 피처 벡터화 유형</h1>\n",
    "\n",
    "- Bag of Words : BOW는 예를 들어서 100개의 문서가 있다면 모든 문서의 단어를 다 추출해서 피쳐로 만듦. 그 다음에 다큐멘터들이 이런 피쳐들을 값으로 가짐. 개별 단어들이 이런 피쳐에 대해서 값을 어떻게 설정하냐면 이 단어가 나타난 횟수가 몇번이냐. doc1은 단어1이 3번 나왔고 2는 4번. 이렇게 횟수를 표현 Document Term Matrix : Term은 단어를 말함.\n",
    "\n",
    "\n",
    "- Word Embedding: Word Embedding은 똑같이 100개의 문서가 있따면 N개의 차원을 만듦. D1, D2, D3, D4, D5 이런식으로 만들어진 차원축에 기반해서 개별 단어들을 벡터 축으로 가져갈 수 있음. 4개의 단어만 있다고 가정을 해보면, X,Y 두개의 축, 직관적으로 봐도 Queen과 WOMNAN, KING과 MAN의 관계를 보면 x 값이 같으므로 유사하다고 볼 수 있음. \n",
    "\n",
    "<img src=\"./img/TextVectorization1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bag of Words - BOW</h1><br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/BOW1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bag-of-Words 구조</h2><br>\n",
    "\n",
    "개별 단어를 bag에다 다 넣고 섞고 중복을 제거한다. 그리고 각 단어에 고유 인덱스 위치 값을 다 부여한다. 그리고 개별 단어에 대해서 각 문장들이 예를 들어서 Counter Vectorization을 한다고하면 and는 문장1에서 한번, baseball에서 2번 등 이런식으로.\n",
    "\n",
    "헷갈릴 수 잇는 것: 어떨때는 왜 문장이고 어떨때는 문서인가? 그건 업무마다 다르다. \n",
    "\n",
    "<img src=\"./img/Bag_of_Words1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BOW 장단점</h2><br>\n",
    " \n",
    "\n",
    "- 장점: Word Embedding보다 빠르다.굳이 '예상보다' 문서의 특징을 잘 타나낸다고 쓴 이유는, 아직도 쓸만하기 때문. Word2vec을 쓰나 BOW를 쓰나 말이 많은데 경험적으로 말하자면 현재 기준에선 BOW가 그렇게까지 떨어지 않는다. 오히려 비슷하거나 더 높을때도 많다.\n",
    "\n",
    "- 단점으론 희소 행렬 문제가 있는데 피쳐가 몇만개씩 나오고 그런다면 하나에 문서에 몇만개의 단어를 다 쓰진 않다. 5만개중 5천개라고 했을때 10%만 데이터가 차고 나머지는 다 안찰 것이다. 이걸 Null로 둘 수 없으니까 0으로 채우면 메모리 문제가 생기도 하고 그렇다.\n",
    " \n",
    "<img src=\"./img/BOW2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BOW 피쳐 벡터화(Feature Vectorization)</h1><br>\n",
    "\n",
    "이렇게 Doc이 M개가 있으면 M x N개의 피처가 벡터화 된다\n",
    "\n",
    "<img src=\"./img/Feature_Vectorization1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BOW 피처 벡터화 유형</h2><br>\n",
    "\n",
    "\n",
    "단순 카운트 기반의 벡터화의 문제: 가령 문서 1에서 어떤 단어가 횟수가 많이 나왔다. 20번 정도. 그럼 중요한 단어로 인식할 수 있다. 그러나 글을 쓸때 생각해보면 딱히 그렇지 않다. 예를 들어 증권이라고 해보자. 거기서 자주 나오는 단어를 봤더니 주가 싯가 종가 등 기본적인 증권 업무의 단어가 많이 나온다고 보자. 이걸 중요한 단어로 파악해서 정보량을 많이 주게 되면 정확한 머신러닝 알고리즘이 돌 수 잇는 중요한 피쳐 역할을 못해준다. 가령 증권영역에 다양한 서브 영역별로 카테고리를 나누고 싶어도 나놀 수 가 없다.\n",
    "\n",
    "보통 문서의 크기가 크고 문서가 굉장히 많을때는 TF-IDF를 적용함. 여기서는 카운트 기반의 벡터 문제같은 단어를 페널티를 준다. 많은 문서에서 똑같이 반복되서 나오면 NOK(not ok)\n",
    "\n",
    "<img src=\"./img/BOWFeatureVectorization1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF(Term Frequency Inverse Document Frequency)</h2><br>\n",
    "\n",
    "특정 단어가 다른 문서에 없고 특정 문서에만 자주 사용 된다면 중요단어.\n",
    "\n",
    "예를 들어 증권이라면, '서킷 브레이크' 라던가. \n",
    "\n",
    "IDF는 DF의 역수이다. 근데 그냥 역수가 아니라 전체문서수/DF 즉 N/DF\n",
    "\n",
    "TF는 높을수록 좋은게 맞다. 그런데 TF도 놓지만 DF도 높다면 분모가 커지니까 값이 작아지게 된다. \n",
    "\n",
    "가령 예를 들어서 DF가 4인데 전체문서가 4 나오면 log 4/4 = log 1=  0 이 되어버린다.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/BOWFeatureVectorization2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>사이킷런 CounterVectorizer 초기화 파라미터</h1><br>\n",
    "\n",
    "- max_df: 문서가 3개 있다고 해보자. 특정단어가 문서1=30 문서2=50 문서3=21 번 나왔다고 하자. max_df=100으로 정수값을 주게 되면, 특정 단어가 101번 전체 문서에 걸쳐서 나오면 이 피쳐는 삭제(즉 필터링)되는거임. 만약 소수점으로 주게 되면 max_df=0.95라면 전체의 95%까지만 던아로 추출하고 상위 5%는 추출하지 않는다. 너무 자주 반복되는 단어들을 필터링하고 싶을때는 max_df를 쓴다.\n",
    "\n",
    "- min_df: 이거는 거의 안나오는 단어를 필터링. 너무 낮으면 garbage 성이다\n",
    "\n",
    "- max_features: 이거는 아예 숫자를 지정하는 것. \n",
    "\n",
    "- stop_words: 스톱 워드로 지정된 것 필터링.\n",
    "\n",
    "- ngram_range: n-gram 형태로 파싱하기 위한건데 살짝 다르다. range 때문에 다름. 디폴트는 (1,1)이다. (1,2)로 하면 이게 바로 레인지이다. 1도 포함을 하고 2도 포함하라는거임.. 앞에 배웠던 바이그램만 시키려면 (2,2)를 해야한다.\n",
    "\n",
    "- analyzer\n",
    "\n",
    "- token_patter\n",
    "\n",
    "- lower_case\n",
    "\n",
    "<img src=\"./img/CounterVectorizer1.png\"/>\n",
    "\n",
    "<img src=\"./img/CounterVectorizer2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Bag of Words – BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사이킷런 CountVectorizer 테스트**\n",
    "\n",
    "두개의 문장을 만들 것임. 그것을 리스트로 하나로 합쳤다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.                   You can see it out your window or on your television.                   You feel it when you go to work, or go to church or pay your taxes.', 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                  You can see it out your window or on your television. \\\n",
    "                  You feel it when you go to work, or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "text=[]\n",
    "text.append(text_sample_01); text.append(text_sample_02)\n",
    "print(text,\"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer객체 생성 후 fit(), transform()으로 텍스트에 대한 feature vectorization 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_vect = cnt_vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**피처 벡터화 후 데이터 유형 및 여러 속성 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 51)\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(ftr_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면 type이 csr_matrix이다. 희소행렬을 물리적으로 공간을 절약하고 개선을 빨리하기 위한 유형임.\n",
    "\n",
    "값을 까보니까 뭔가 모르는 (0, 0) ~ 등이 있는데 이게 바로 각각 단어들의 위치를 말해주는 것임. 그 오른쪽의 숫자 1은 다큐먼트 1의 1번째 것이 1번 나왔다는 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**행 값이 어떤게 나왔는지 알려면 .vocabulary_**\n",
    "\n",
    "이 속성을 확인하면 index가 나온다. the 는 38번 index, matrix는 22번째 index 라는 뜻.\n",
    "\n",
    "위에 찾아보면 (0,22) 1 인데 matrix가 첫번째 문서에 1번 나왔다는 뜻임.\n",
    "\n",
    "(1, 22) 가 없으므로 두번째 문서에서는 안나왔다는 뜻임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'window': 4, 'pill': 1, 'wake': 2, 'believe': 0, 'want': 3}\n"
     ]
    }
   ],
   "source": [
    "# cnt_vect = CountVectorizer(max_features=5) # stop_words를 안했더니 의미없는 the, you, your 등이 나옴\n",
    "cnt_vect = CountVectorizer(max_features=5, stop_words='english')\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ngram_range 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 201)\n",
      "{'the': 129, 'matrix': 77, 'is': 66, 'everywhere': 40, 'its': 74, 'all': 0, 'around': 11, 'us': 150, 'here': 51, 'even': 37, 'in': 59, 'this': 140, 'room': 106, 'you': 174, 'can': 25, 'see': 109, 'it': 69, 'out': 90, 'your': 193, 'window': 165, 'or': 83, 'on': 80, 'television': 126, 'feel': 43, 'when': 162, 'go': 46, 'to': 143, 'work': 171, 'church': 28, 'pay': 93, 'taxes': 125, 'the matrix': 132, 'matrix is': 78, 'is everywhere': 67, 'everywhere its': 41, 'its all': 75, 'all around': 1, 'around us': 12, 'us here': 151, 'here even': 52, 'even in': 38, 'in this': 60, 'this room': 141, 'room you': 107, 'you can': 177, 'can see': 26, 'see it': 110, 'it out': 70, 'out your': 91, 'your window': 199, 'window or': 166, 'or on': 86, 'on your': 81, 'your television': 197, 'television you': 127, 'you feel': 179, 'feel it': 44, 'it when': 72, 'when you': 163, 'you go': 181, 'go to': 47, 'to work': 148, 'work or': 172, 'or go': 84, 'to church': 146, 'church or': 29, 'or pay': 88, 'pay your': 94, 'your taxes': 196, 'the matrix is': 133, 'matrix is everywhere': 79, 'is everywhere its': 68, 'everywhere its all': 42, 'its all around': 76, 'all around us': 2, 'around us here': 13, 'us here even': 152, 'here even in': 53, 'even in this': 39, 'in this room': 61, 'this room you': 142, 'room you can': 108, 'you can see': 178, 'can see it': 27, 'see it out': 111, 'it out your': 71, 'out your window': 92, 'your window or': 200, 'window or on': 167, 'or on your': 87, 'on your television': 82, 'your television you': 198, 'television you feel': 128, 'you feel it': 180, 'feel it when': 45, 'it when you': 73, 'when you go': 164, 'you go to': 182, 'go to work': 49, 'to work or': 149, 'work or go': 173, 'or go to': 85, 'go to church': 48, 'to church or': 147, 'church or pay': 30, 'or pay your': 89, 'pay your taxes': 95, 'take': 121, 'blue': 22, 'pill': 96, 'and': 3, 'story': 118, 'ends': 34, 'wake': 153, 'bed': 14, 'believe': 17, 'whatever': 159, 'want': 156, 'red': 103, 'stay': 115, 'wonderland': 168, 'show': 112, 'how': 56, 'deep': 31, 'rabbit': 100, 'hole': 54, 'goes': 50, 'you take': 187, 'take the': 122, 'the blue': 130, 'blue pill': 23, 'pill and': 97, 'and the': 6, 'the story': 138, 'story ends': 119, 'ends you': 35, 'you wake': 189, 'wake in': 154, 'in your': 64, 'your bed': 194, 'bed and': 15, 'and you': 8, 'you believe': 175, 'believe whatever': 18, 'whatever you': 160, 'you want': 191, 'want to': 157, 'to believe': 144, 'believe you': 20, 'the red': 136, 'red pill': 104, 'you stay': 185, 'stay in': 116, 'in wonderland': 62, 'wonderland and': 169, 'and show': 4, 'show you': 113, 'you how': 183, 'how deep': 57, 'deep the': 32, 'the rabbit': 134, 'rabbit hole': 101, 'hole goes': 55, 'you take the': 188, 'take the blue': 123, 'the blue pill': 131, 'blue pill and': 24, 'pill and the': 98, 'and the story': 7, 'the story ends': 139, 'story ends you': 120, 'ends you wake': 36, 'you wake in': 190, 'wake in your': 155, 'in your bed': 65, 'your bed and': 195, 'bed and you': 16, 'and you believe': 9, 'you believe whatever': 176, 'believe whatever you': 19, 'whatever you want': 161, 'you want to': 192, 'want to believe': 158, 'to believe you': 145, 'believe you take': 21, 'take the red': 124, 'the red pill': 137, 'red pill and': 105, 'pill and you': 99, 'and you stay': 10, 'you stay in': 186, 'stay in wonderland': 117, 'in wonderland and': 63, 'wonderland and show': 170, 'and show you': 5, 'show you how': 114, 'you how deep': 184, 'how deep the': 58, 'deep the rabbit': 33, 'the rabbit hole': 135, 'rabbit hole goes': 102}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(ngram_range=(1,3)) # 1도하고 2도하고 3도 하라는 것임. 1 + 2 + 3\n",
    "# cnt_vect = CountVectorizer(ngram_range=(1,2)) # 디폴트로 1도 하고 2도 하라는 얘기임. 1 + 2 를 하는 것이다. the matrix부터 바이그램하기 시작했다.\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>희소행렬의 이해</h1><br>\n",
    "<h2>CounterVectorizer를 이용한 피처 벡터화</h2><br>\n",
    "\n",
    "벡터화했더니 익숙하지 않은 CSR_matrix같은게 나온는데 이게 휘소행렬이다.\n",
    "\n",
    "수천 수만개 문서가 있다면 모든 단어를 다 나열하기 떄문에 굉장히 많은 단어들이 나온다.\n",
    "\n",
    "개별문서가 그 단어를 가지고 있을 확률이 일부만 가지고 있을거다. 사전도 아니고.\n",
    "\n",
    "그렇기 떄문에 BOW피쳐 벡터화 모델은 너무 많은 0값이 메모리에 할당이되기 때문에 많은 메모리 공간도 필요하고 연산 속도에도 영향을 미친다.\n",
    "\n",
    "그러므로 희소행렬을 변환을 하는데 COO형식과 CSR형식이 있다. 0이 아닌 값만 저장하고 그 값들의 위치만 기억하면 된다.\n",
    "\n",
    "<img src=\"./img/CounterVectorizer3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>희소행렬</h1><br>\n",
    "\n",
    "<img src=\"./img/COOCSR1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>희소행렬의 저장 변환 공식</h2><br>\n",
    "\n",
    "COO, CSR 형식\n",
    "\n",
    "굳이 다 이해할 필요는 없고 이런게 있다하고 상식선에서만 생각하면 된다.\n",
    "\n",
    "<img src=\"./img/COOCSR2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>COO 형식</h2><br>\n",
    "\n",
    "COO 형식은  0이 아닌 데이터 값만 추출해서 그 행과 열 위치를 추출하여 그 인덱스를 기억하는 것\n",
    "\n",
    "<img src=\"./img/COO1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CSR 형식</h2><br>\n",
    "\n",
    "COO형식의 문제인, 행 위치 배열의 중복값들도 커질 수 있으므로 행 위치배열을 어떻게 바꾸냐면 행위치 배열의 위치 배열 인덱스(위치의 위치. 인덱스의 인덱스)\n",
    "\n",
    "총 항목 개수 배열을 가지면 그게 13이면 12가 마지막임을 알 수 있다. 훨씬 더 간단하게 메모리를 줄일 수 있다.\n",
    "\n",
    "<img src=\"./img/CSR1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 행렬이 아래와 같이 있다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array( [ [ 3, 0, 1 ], \n",
    "                    [0, 2, 0 ] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n",
      "<class 'numpy.ndarray'> \n",
      " [[3 0 1]\n",
      " [0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(sparse_coo))\n",
    "print(sparse_coo)\n",
    "dense01=sparse_coo.toarray() # 다시 원본 행렬로 반환하려면 toarray()를 하면 된다.\n",
    "print(type(dense01),\"\\n\", dense01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 행렬 – CSR 형식\n",
    "\n",
    "이것도 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그냥 간단하게 아래와 같이도 가능하다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
