{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 NLP 한글 NLP 처리의 어려움\n",
    "\n",
    "영어 자연어처리에 비해서 컴퓨터에서 처리하는게 상대적으로 어렵다.\n",
    "\n",
    "띄워 쓰기, 다양한 조사 등. 특히 띄워 쓰기는 영어 같은 경우는 명확한데 한글은 안 그런 경우도 많다.\n",
    "\n",
    "띄워 쓰기를 잘못해도 의미전달이 잘 되는 경우도 많다. 차라리 오류가 생기면 좋은데.\n",
    "\n",
    "조사를 분리하기도 쉽지 않다.\n",
    "\n",
    "주어 목적어가 생략되어도 의미 전달이 가능하다. 밥 먹었니? 같이.\n",
    "\n",
    "의성어 의태어 높임말도 많다. \n",
    "\n",
    "한가지 첨언. 영어 NLP는 오랜 시간동안 많은 리소스가 투입되서 연구가 이루어졌다. 그런데 영어 자연어처리를 하기 위해서\n",
    "\n",
    "정부적인 차원, 학계, 등의 노력이 있었다.\n",
    "\n",
    "\n",
    "\n",
    "![](./img/KoNLP1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 형태소 분석\n",
    "\n",
    "**'단어로서의 의미를 가지는 최소 단위'로 정의할 수 있다.** 최소 단위에 유의!\n",
    "\n",
    "\n",
    "단어로써 더 이상 쪼개지지 않는 최소 단위.\n",
    "\n",
    "\"머릿결\"의 형태소는? 머리 + 결\n",
    "\n",
    "\"너를\"의 형태소는? 너 + 를(을)\n",
    "\n",
    "\"첫째\"의 형태소는? 첫 + 째\n",
    "\n",
    "형태소 분석이란 말뭉치를 이러한 형태소 어근 단위로 쪼개고 각 형태소에 품사 태깅(POS tagging)을 부착하는 작업을 일반적으로 지칭\n",
    "\n",
    "\n",
    "\n",
    "![](./img/KoNLP2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoNLPy 소개\n",
    "\n",
    "KoNLPy는 기존의 형태소 엔진들이 C와 C++, Java 로 만들어져 있었다. 이걸 파이썬에서 호출할 수 있게 래퍼를 씌우는 것임.\n",
    "\n",
    "대표적인 한글 형태소 엔진이\n",
    "\n",
    "* 꼬꼬마(KKma)\n",
    "* 한나눔(Hannanum()\n",
    "* komoran\n",
    "* 은전한닢 프로젝트(Mecab)\n",
    "* Twitter\n",
    "\n",
    "설치문서 KoNLPy\n",
    "\n",
    "[KoNLPy 설치문서 바로 가기 링크](https://konlpy-ko.readthedocs.io/ko/v0.4.3/install/)\n",
    "\n",
    "![](./img/KoNLPy1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 윈도우 환경에서 KoNLPy 설치하기\n",
    "\n",
    "윈도우 환경에서 KoNLPy 설치하기가 살짝 힘들다.\n",
    "\n",
    "자바 클래스를 파이썬에서 호출할 수 있는 모듈이 필요한데 이 설치를 pip로하면 복잡하고 conda로하면 깔끔하다.\n",
    "\n",
    "Jpype (제이 파이프)는 자바클래스를 파이썬에서 호출할 수 있게 하는 것\n",
    "\n",
    "Java와 Jpype 설치 순서는 상관없다 \n",
    "\n",
    "jdk를 받는 방법이 있고 jre를 받는 방법이 있다. jdk는 개발을 위한 모든게 다 들어가 있는거라 여기는 jre를 받도록 하겠음\n",
    "\n",
    "설치 경로는 Program Files 에 Java 폴더에 있다\n",
    "\n",
    "자바홈 설정 할때, 공식 홈페이지에 있는 방식으로 하면 실행이 안된다. (원래 일반적으로는 공식문서대로 하는게 맞지만)\n",
    "\n",
    "그래서 어떻게 하냐면, \n",
    "\n",
    "C:\\Program Files\\Java\\jre1.8.0_271\\bin\\server \n",
    "\n",
    "이렇게 jvm.dll 파일이 있는 서브 디렉토리까지 지정을 해야한다.\n",
    "\n",
    "이걸 하기전에 혹시 환경 파라미터 부터 지정하자. \n",
    "\n",
    "윈도우에서 자바 언더바 홈은 내 피씨에 가서 속성, 고급시스템 설정에 들어가면 환경변수가 있다.\n",
    "\n",
    "여기에 새로만들기를 하나 누르면 된다.그리고 대문자로 JAVA_HOME을 넣어준다.\n",
    "\n",
    "그리고 jvm.dll이 있는 디렉토리 주소복사를 한다. 변수 값에 넣어준다.\n",
    "\n",
    "자바 홈이 이미 잡혀있는 사람은 의문이 들 수 있을텐데, KoNLPy 쓸 때만 이렇게 지정해주자\n",
    "\n",
    "그 다음에 pip 인스톨을 하면 된다.\n",
    "\n",
    "![](./img/KoNLP_window.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver sentiment movie corpus\n",
    "\n",
    "흔치 않은 한글 데이터 분석 세트이다. 네이버 영화리뷰 감성 분석 데이터.\n",
    "\n",
    "1이 긍정, 0이 부정이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                           document  label\n",
       "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('ratings_train.txt', sep='\\t') #seperate는 Tab 키\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75173\n",
       "1    74827\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 보면 Null 값이 5개 정도 있는데 얼마 안되니까 그냥 공백으로 바꾸겠음.\n",
    "\n",
    "그 다음에 리뷰 글 안에 있는 숫자들을 다 없애버릴 것임\n",
    "\n",
    "파이썬 정규표현식을 쓸건데, 텍스트처리를 할거면 꼭 알아둬야 함. 굉장히 유용함\n",
    "\n",
    "re는 regular expression의 약자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train_df = train_df.fillna(' ')\n",
    "# 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
    "train_df['document'] = train_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
    "train_df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
    "test_df = pd.read_csv('ratings_test.txt', sep='\\t')\n",
    "test_df = test_df.fillna(' ')\n",
    "test_df['document'] = test_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
    "test_df.drop('id', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 앞에서 설치한 konlpy로 트위터의 형태소 분석을 할 것임.\n",
    "\n",
    "트위터의 morphs가 형태소 분석을 해주는 메소드이다. 여기에 인자로 text로 넣어주면\n",
    "\n",
    "각각의 형태소를 리스트 형태로 반환한다. \n",
    "\n",
    "그리고 tw_tokenizer를 새로 만드는데 이걸 왜로 만드냐면 뒤에서 쓰려고 한다. 한번 테스트 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['첫째']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "def tw_tokenizer(text):\n",
    "    # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\n",
    "    tokens_ko = twitter.morphs(text)\n",
    "    return tokens_ko\n",
    "\n",
    "tw_tokenizer('첫째')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 TfidVectorizer로 피쳐 벡터화를 할 것임. \n",
    "\n",
    "뭐를 썼냐면 이걸 이용해서 피처 벡터화를 할건데 토크나이저의 형태소 분석함수를 넣을 것임. 위의 tw_tokenizer를 보면 tokens_ko를 반환함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Twitter 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2) \n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(train_df['document'])\n",
    "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드 설명\n",
    "\n",
    "C 값은 LogisticRegression의 알파의 역수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   33.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.5} 0.8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esaw2\\Anaconda3\\envs\\pygta5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression 을 이용하여 감성 분석 Classification 수행. \n",
    "lg_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# Parameter C 최적화를 위해 GridSearchCV 를 이용. \n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lg_clf , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
    "grid_cv.fit(tfidf_matrix_train , train_df['label'] )\n",
    "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드에서 \n",
    "\n",
    "{'C': 3.5} 0.8592 \n",
    "\n",
    "이렇게 나옴. 세개의 폴드 셋으로 해본 결과 평균 0.8592가 나오고 C값은 3.5가 최적화된 하이퍼파라미터로 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 C값 3.5를 logisticsregression을 적용한 best_estimator객체를 이용해서 학습을 \n",
    "\n",
    "할 건데 test 데이터셋을 피쳐 벡터화할 때는 한가지 유의할 점이 있다고 이전에 얘기했다. \n",
    "\n",
    "뭐냐면 tfidf_vect 요 객체 변수가 앞에서 TfidVectorizer에서  학습데이터를 fit한 개체이다. \n",
    "\n",
    "그러므로 test데이터를 할 때 다시 fit을 하면 안된다. 잘 모르겠으면 텍스트 분류할때 \n",
    "\n",
    "왜 test데이터는 앞에 train 데이터로 학습된 Tfidf로 해야되는지 얘기했으므로 참고하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 정확도:  0.86184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 학습 데이터를 적용한 TfidfVectorizer를 이용하여 테스트 데이터를 TF-IDF 값으로 Feature 변환함. \n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
    "\n",
    "# classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "\n",
    "print('Logistic Regression 정확도: ',accuracy_score(test_df['label'],preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_range를 1,3으로 하면 시간이 더 늘어남. 리눅스에서 하면 은전한닢으로도 해보길 권함. tw_tokenizer 해야되는 부분 유의."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
